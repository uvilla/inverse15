{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\def\\data{\\bf d_\\rm{obs}}\n",
      "\\def\\vec{\\bf}\n",
      "\\def\\m{\\bf m}\n",
      "\\def\\map{\\bf m_{\\text{MAP}}}\n",
      "\\def\\postcov{\\bf \\Gamma_{\\text{post}}}\n",
      "\\def\\prcov{\\bf \\Gamma_{\\text{prior}}}\n",
      "\\def\\matrix{\\bf}\n",
      "\\def\\Hmisfit{\\bf H_{\\text{misfit}}}\n",
      "\\def\\HT{\\tilde{\\bf H}_{\\text{misfit}}}\n",
      "\\def\\diag{diag}\n",
      "\\def\\Vr{\\matrix V_r}\n",
      "\\def\\Wr{\\matrix W_r}\n",
      "\\def\\Ir{\\matrix I_r}\n",
      "\\def\\Dr{\\matrix D_r}\n",
      "\\def\\H{\\matrix H}\n",
      "$$ \n",
      "# Example: Bayesian quantification of parameter uncertainty:\n",
      "## Estimating the (Gaussian) posterior pdf of the coeffcient parameter field in an elliptic PDE\n",
      "\n",
      "In this example we tackle the problem of quantifying the\n",
      "uncertainty in the solution of an inverse problem governed by an\n",
      "elliptic PDE via the Bayesian inference framework. \n",
      "Hence, we state the inverse problem as a\n",
      "problem of statistical inference over the space of uncertain\n",
      "parameters, which are to be inferred from data and a physical\n",
      "model.  The resulting solution to the statistical inverse problem\n",
      "is a posterior distribution that assigns to any candidate set of\n",
      "parameter fields our belief (expressed as a probability) that a\n",
      "member of this candidate set is the ``true'' parameter field that\n",
      "gave rise to the observed data.\n",
      "\n",
      "For simplicity, in what follows we give finite-dimensional expressions (i.e., after\n",
      "discretization of the parameter space) for the Bayesian\n",
      "formulation of the inverse problem.\n",
      "\n",
      "### Bayes' Theorem:\n",
      "\n",
      "The posterior probability distribution combines the prior pdf\n",
      "$\\pi_{\\text{prior}}(\\m)$ over the parameter space, which encodes\n",
      "any knowledge or assumptions about the parameter space that we may\n",
      "wish to impose before the data are considered, with a likelihood pdf\n",
      "$\\pi_{\\text{like}}(\\vec{d}_{\\text{obs}} \\; | \\; \\m)$, which explicitly\n",
      "represents the probability that a given set of parameters $\\m$\n",
      "might give rise to the observed data $\\vec{d}_{\\text{obs}} \\in\n",
      "\\mathbb{R}^m$, namely:\n",
      "\n",
      "$\n",
      "\\begin{align}\n",
      "\\pi_{\\text{post}}(\\m | \\data) \\propto\n",
      "\\pi_{\\text{prior}}(\\m) \\pi_{\\text{like}}(\\data | \\m).\n",
      "\\end{align}\n",
      "$\n",
      "\n",
      "Note that the infinite-dimensional analog of Bayes' formula cannot be stated  formulated  using pdfs but requires Radon-Nikodym derivatives\n",
      "\n",
      "### Gaussian prior and noise:\n",
      "\n",
      "#### The prior:\n",
      "\n",
      "We consider a Gaussian prior with mean $\\vec m_{\\text prior}$ and covariance $\\bf \\Gamma_{\\text{prior}}$. The covariance is given by the discretization of the inverse of differential operator $\\mathcal{A}^{-2} = (-\\gamma \\Delta + \\delta I)^{-2}$, where $\\gamma$, $\\delta > 0$ control the correlation length and the variance of the prior operator. This choice of prior ensures that it is a trace-class operator, guaranteeing bounded pointwise variance and a well-posed infinite-dimensional Bayesian inverse problem\n",
      "\n",
      "#### The likelihood:\n",
      "\n",
      "$\n",
      "\\data =  \\bf{f}(\\m) + \\bf{e }, \\;\\;\\;  \\bf{e} \\sim \\mathcal{N}(\\bf{0}, \\bf \\Gamma_{\\text{noise}} )\n",
      "$\n",
      "\n",
      "$\n",
      "\\pi_{\\text like}(\\data \\; | \\; \\m)  = \\exp \\left( - \\tfrac{1}{2} (\\bf{f}(\\m) - \\data)^T \\bf \\Gamma_{\\text{noise}}^{-1} (\\bf{f}(\\m) - \\data)\\right)\n",
      "$\n",
      "\n",
      "Here $\\bf f$ is the parameter-to-observable map that takes a parameter vector $\\m$ and maps\n",
      "it to the space observation vector $\\data$.\n",
      "\n",
      "#### The posterior:\n",
      "\n",
      "$\n",
      "\\pi_{\\text{post}}(\\m \\; | \\; \\data)  \\propto \\exp \\left( - \\tfrac{1}{2} \\parallel \\bf{f}(\\m) - \\data \\parallel^{2}_{\\bf  \\Gamma_{\\text{noise}}^{-1}} \\! - \\tfrac{1}{2}\\parallel \\m - \\m_{\\text prior} \\parallel^{2}_{\\bf \\Gamma_{\\text{prior}}^{-1}} \\right)\n",
      "$\n",
      "\n",
      "### The Gaussian approximation of the posterior: $\\mathcal{N}(\\vec{\\map},\\bf \\Gamma_{\\text{post}})$\n",
      "\n",
      "The mean of this posterior distribution, $\\vec{\\map}$, is the\n",
      "parameter vector maximizing the posterior, and\n",
      "is known as the maximum a posteriori (MAP) point.  It can be found\n",
      "by minimizing the negative log of the posterior, which amounts to\n",
      "solving a deterministic inverse problem) with appropriately weighted norms,\n",
      "\n",
      "$\n",
      "\\map := \\underset{\\m}{\\arg \\min} \\; \\mathcal{J}(\\m) \\;:=\\;\n",
      "\\Big( \n",
      "-\\frac{1}{2} \\| \\bf f(\\m) - \\data \\|^2_{\\bf \\Gamma_{\\text{noise}}^{-1}} \n",
      "-\\frac{1}{2} \\| \\m -\\m_{\\text prior} \\|^2_{\\bf \\Gamma_{\\text{prior}}^{-1}} \n",
      "\\Big).\n",
      "$\n",
      "\n",
      "The posterior covariance matrix is then given by the inverse of\n",
      "the Hessian matrix of $\\mathcal{J}$ at $\\map$, namely\n",
      "\n",
      "$\n",
      "\\bf \\Gamma_{\\text{post}} = \\left(\\Hmisfit(\\map) + \\bf \\Gamma_{\\text{prior}}^{-1} \\right)^{-1} \n",
      "= \\left(\\prcov \\Hmisfit + \\matrix{I}\\right)^{-1}\\prcov\n",
      "$\n",
      "\n",
      "#### The prior-preconditioned Hessian of the data misfit:\n",
      "\n",
      "$\n",
      "  \\HT := \\prcov \\Hmisfit\n",
      "$\n",
      "\n",
      "#### The generalized eigenvalue problem:\n",
      "\n",
      "$\n",
      " \\Hmisfit \\matrix{W} = \\prcov^{-1} \\matrix{W} \\matrix{\\Lambda},\n",
      "$\n",
      "\n",
      "where $\\matrix{\\Lambda} = diag(\\lambda_i) \\in \\mathbb{R}^{n\\times n}$\n",
      "contains the generalized eigenvalues and the columns of $\\matrix W\\in\n",
      "\\mathbb R^{n\\times n}$ the generalized eigenvectors such that \n",
      "$\\matrix{W}^T \\prcov^{-1} \\matrix{W} = \\matrix{I}$. Defining \n",
      "$\\matrix V := \\prcov^{-1}\\matrix W$\n",
      "\n",
      "$\n",
      "\\prcov \\Hmisfit = \\matrix{W} \\matrix{\\Lambda} \\matrix{V}^T.\n",
      "$\n",
      "\n",
      "#### Randomized SVD algorithms to construct the approximate spectral decomposition:  \n",
      "\n",
      "When the generalized eigenvalues $\\{\\lambda_i\\}$ decay rapidly, we can\n",
      "extract a low-rank approximation of $\\HT$ by retaining only the $r$\n",
      "largest eigenvalues and corresponding eigenvectors,\n",
      "\n",
      "$\n",
      "\\HT \\approx \\matrix{W}_r \\matrix{\\Lambda}_r \\matrix{V}_r^T.\n",
      "$\n",
      "\n",
      "Here, $\\matrix{W}_r \\in \\mathbb{R}^{n\\times r}$ contains only the $r$\n",
      "eigenvectors of $\\HT$ that correspond to the $r$ largest eigenvalues,\n",
      "which are assembled into the diagonal matrix $\\matrix{\\Lambda}_r = \\diag\n",
      "(\\lambda_i) \\in \\mathbb{R}^{r \\times r}$, and\n",
      "$\\matrix{V}_r=\\prcov^{-1} \\matrix{W}_r$.\n",
      "\n",
      "#### Invert with the Sherman-Morrison-Woodbury formula:\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "  \\notag \\left(\\HT+ \\matrix{I}\\right)^{-1}\n",
      "  = \\matrix{I}-\\matrix{W}_r \\matrix{D}_r \\matrix{V}_r^T +\n",
      "  \\mathcal{O}\\left(\\sum_{i=r+1}^{n} \\frac{\\lambda_i}{\\lambda_i +\n",
      "    1}\\right),\n",
      "\\end{align}\n",
      "$$\n",
      "\n",
      "where $\\matrix{D}_r :=\\diag(\\lambda_i/(\\lambda_i+1)) \\in\n",
      "\\mathbb{R}^{r\\times r}$. The last term in this expression captures the\n",
      "error due to truncation in terms of the discarded eigenvalues; this\n",
      "provides a criterion for truncating the spectrum, namely that $r$ is\n",
      "chosen such that $\\lambda_r$ is small relative to 1. \n",
      "\n",
      "#### The approximate posterior covariance:\n",
      "\n",
      "$$\n",
      "\\postcov \\approx (\\matrix{I} - \\matrix{W}_r \\matrix{D}_r\n",
      "\\matrix{V}_r^T) \\prcov = \n",
      "\\prcov\n",
      "- \\matrix{W}_r \\matrix{D}_r \\matrix{W}_r^T\n",
      "$$\n",
      "\n",
      "#### Apply the inverse and square-root inverse Hessian to a vector (as needed for drawing samples from a Gaussian distribution with covariance $\\H^{-1}$)\n",
      "\n",
      "$$\n",
      "  \\H^{-1} \\bf v \\approx ( \\matrix{I}-\\Wr \\Dr \\Vr^T)\n",
      "  \\prcov \\bf v =  \\big\\{ \\bf W_r \\big[ (\\matrix{\\Lambda}_r +\n",
      "    \\bf I_r)^{-1} - \\bf I_r \\big]  \\Vr^T + \\bf I \\big\\} \\prcov \\bf v\n",
      "$$\n",
      " \n",
      "$$\n",
      "  \\H^{-1/2} \\bf v \\approx \\big\\{ \\Wr \\big[ (\\matrix{\\Lambda}_r +\n",
      "    \\Ir)^{-1/2} - \\Ir \\big] \\Vr^T  + \\bf I \\big\\} \\prcov^{1/2}\\bf v\n",
      "$$\n",
      "\n",
      "### This tutorial shows:\n",
      "\n",
      "- convergence of the inexact Newton-CG algorithm\n",
      "- low-rank-based approximation of the posterior covariance (built on a low-rank\n",
      "approximation of the Hessian of the data misfit) \n",
      "- how to construct the low-rank approximation of the Hessian of the data misfit\n",
      "- how to apply the inverse and square-root inverse Hessian to a vector efficiently\n",
      "- samples from the Gaussian approximation of the posterior\n",
      "\n",
      "### Goals:\n",
      "\n",
      "By the end of this notebook, you should be able to:\n",
      "\n",
      "- understand the Bayesian inverse framework\n",
      "- visualise and understand the results\n",
      "- modify the problem and code\n",
      "\n",
      "### Mathematical tools used:\n",
      "\n",
      "- Finite element method\n",
      "- Derivation of gradiant and Hessian via the adjoint method\n",
      "- inexact Newton-CG\n",
      "- Armijo line search\n",
      "- Bayes' formula\n",
      "\n",
      "### List of software used:\n",
      "\n",
      "- <a href=\"http://fenicsproject.org/\">FEniCS</a>, a parallel finite element element library for the discretization of partial differential equations\n",
      "- <a href=\"http://www.mcs.anl.gov/petsc/\">PETSc</a>, for scalable and efficient linear algebra operations and solvers\n",
      "- <a href=\"http://matplotlib.org/\">Matplotlib</a>, A great python package that I used for plotting many of the results\n",
      "- <a href=\"http://www.numpy.org/\">Numpy</a>, A python package for linear algebra.  While extensive, this is mostly used to compute means and sums in this notebook."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import dolfin as dl\n",
      "import sys\n",
      "sys.path.append( \"../\" )\n",
      "from hippylib import *\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "from PoissonModel import Poisson\n",
      "\n",
      "import nb\n",
      "\n",
      "import logging\n",
      "logging.getLogger('FFC').setLevel(logging.WARNING)\n",
      "logging.getLogger('UFL').setLevel(logging.WARNING)\n",
      "dl.set_log_active(False)\n",
      "\n",
      "#uncomment this to visualize a list of all the methods available \n",
      "#help(Poisson)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Set up the mesh and finite element spaces\n",
      "\n",
      "We compute a two dimensional mesh of a unit square with nx by ny elements.\n",
      "We define a P2 finite element space for the *state* and *adjoint* variable and P1 for the *parameter*."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ndim = 2\n",
      "nx = 64\n",
      "ny = 64\n",
      "mesh = dl.UnitSquareMesh(nx, ny)\n",
      "Vh2 = dl.FunctionSpace(mesh, 'Lagrange', 2)\n",
      "Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)\n",
      "Vh = [Vh2, Vh1, Vh2]\n",
      "print \"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format(Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Set up the location of observation, Prior Information, and model\n",
      "\n",
      "We observe at *ntargets* random locations. *rel_noise* is the signal to noise ratio."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ntargets = 300\n",
      "np.random.seed(seed=1)\n",
      "targets = np.random.uniform(0.1,0.9, [ntargets, ndim] )\n",
      "print \"Number of observation points: {0}\".format(ntargets)\n",
      "\n",
      "gamma = 2\n",
      "delta = 5\n",
      "prior = BiLaplacianPrior(Vh[PARAMETER], gamma, delta)\n",
      "    \n",
      "print \"Prior regularization: (delta - gamma*Laplacian)^2: delta={0}, gamma={1}\".format(delta, gamma)    \n",
      "    \n",
      "atrue_expression = dl.Expression('log(2+7*(pow(pow(x[0] - 0.5,2) + pow(x[1] - 0.5,2),0.5) > 0.2)) - log(10)')\n",
      "prior_mean_expression = dl.Expression('log(9) - log(10)')\n",
      "    \n",
      "atrue = dl.interpolate(atrue_expression, Vh[PARAMETER]).vector()\n",
      "prior.mean = dl.interpolate(prior_mean_expression, Vh[PARAMETER]).vector()\n",
      "\n",
      "objs = [dl.Function(Vh[PARAMETER],atrue), dl.Function(Vh[PARAMETER],prior.mean)]\n",
      "mytitles = [\"True Parameter\", \"Prior mean\"]\n",
      "nb.multi1_plot(objs, mytitles)\n",
      "plt.show()\n",
      "    \n",
      "rel_noise = 0.01\n",
      "model = Poisson(mesh, Vh, atrue, targets, prior, rel_noise)\n",
      "\n",
      "utrue = model.generate_vector(STATE)\n",
      "model.solveFwd(utrue, [utrue, atrue])\n",
      "\n",
      "vmax = max( utrue.max(), model.u_o.max() )\n",
      "vmin = min( utrue.min(), model.u_o.min() )\n",
      "\n",
      "plt.figure(figsize=(15,5))\n",
      "nb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True State\", subplot_loc=121, vmin=vmin, vmax=vmax)\n",
      "nb.plot_pts(targets, model.u_o, mytitle=\"Observations\", subplot_loc=122, vmin=vmin, vmax=vmax)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Test the gradient and the Hessian of the model\n",
      "\n",
      "We test the gradient and the Hessian of the model using finite differences."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a0 = dl.interpolate(dl.Expression(\"sin(x[0])\"), Vh[PARAMETER])\n",
      "modelVerify(model, a0.vector(), 1e-4, 1e-6)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Compute the MAP point\n",
      "\n",
      "We used the globalized Newtown-CG method to compute the MAP point."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a0 = prior.mean.copy()\n",
      "solver = ReducedSpaceNewtonCG(model)\n",
      "solver.parameters[\"rel_tolerance\"] = 1e-9\n",
      "solver.parameters[\"abs_tolerance\"] = 1e-12\n",
      "solver.parameters[\"max_iter\"]      = 25\n",
      "solver.parameters[\"inner_rel_tolerance\"] = 1e-15\n",
      "solver.parameters[\"c_armijo\"] = 1e-4\n",
      "solver.parameters[\"GN_iter\"] = 5\n",
      "    \n",
      "x = solver.solve(a0)\n",
      "    \n",
      "if solver.converged:\n",
      "    print \"\\nConverged in \", solver.it, \" iterations.\"\n",
      "else:\n",
      "    print \"\\nNot Converged\"\n",
      "\n",
      "print \"Termination reason: \", solver.termination_reasons[solver.reason]\n",
      "print \"Final gradient norm: \", solver.final_grad_norm\n",
      "print \"Final cost: \", solver.final_cost\n",
      "\n",
      "plt.figure(figsize=(15,5))\n",
      "nb.plot(dl.Function(Vh[STATE], x[STATE]), subplot_loc=121,mytitle=\"State\")\n",
      "nb.plot(dl.Function(Vh[PARAMETER], x[PARAMETER]), subplot_loc=122,mytitle=\"Parameter\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Compute the low rank Gaussian approximation of the posterior\n",
      "We used the *single pass* algorithm to compute a low-rank decomposition of the Hessian Misfit.\n",
      "In particular, we solve\n",
      "\n",
      "$$ H_{\\rm misfit} u = \\lambda R u. $$\n",
      "\n",
      "The Figure shows the largest *k* generalized eigenvectors of the Hessian misfit.\n",
      "The effective rank of the Hessian misfit is the number of eigenvalues above the red line (y=1).\n",
      "The effective rank is independent of the mesh size."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.setPointForHessianEvaluations(x)\n",
      "Hmisfit = ReducedHessian(model, solver.parameters[\"inner_rel_tolerance\"], gauss_newton_approx=False, misfit_only=True)\n",
      "k = 20\n",
      "p = 10\n",
      "print \"Single/Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p)\n",
      "Omega = np.random.randn(x[PARAMETER].array().shape[0], k+p)\n",
      "#d, U = singlePassG(Hmisfit, prior.R, prior.Rsolver, Omega, k)\n",
      "d, U = doublePassG(Hmisfit, prior.R, prior.Rsolver, Omega, k)\n",
      "\n",
      "posterior = GaussianLRPosterior(prior, d, U)\n",
      "posterior.mean = x[PARAMETER]\n",
      "\n",
      "#d2, U2 = singlePass(Hmisfit, Omega, k)\n",
      "\n",
      "plt.plot(range(0,k), d, 'b*', range(0,k+1), np.ones(k+1), '-r')\n",
      "plt.yscale('log')\n",
      "plt.xlabel('number')\n",
      "plt.ylabel('eigenvalue')\n",
      "\n",
      "nb.plot_eigenvectors(Vh[PARAMETER], U, mytitle=\"Eigenvector\", which=[0,1,2,5,10,15])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Prior and posterior pointwise variance fields"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "compute_trace = False\n",
      "if compute_trace:\n",
      "    post_tr, prior_tr, corr_tr = posterior.trace(method=\"Estimator\", tol=5e-2, min_iter=20, max_iter=200)\n",
      "    print \"Posterior trace {0:5e}; Prior trace {1:5e}; Correction trace {2:5e}\".format(post_tr, prior_tr, corr_tr)\n",
      "post_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(\"Exact\")\n",
      "\n",
      "objs = [dl.Function(Vh[PARAMETER], pr_pw_variance),\n",
      "        dl.Function(Vh[PARAMETER], post_pw_variance)]\n",
      "mytitles = [\"Prior variance\", \"Posterior variance\"]\n",
      "nb.multi1_plot(objs, mytitles, logscale=True)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Generate samples from Prior and Posterior"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nsamples = 5\n",
      "noise = dl.Vector()\n",
      "posterior.init_vector(noise,\"noise\")\n",
      "noise_size = noise.array().shape[0]\n",
      "s_prior = dl.Function(Vh[PARAMETER], name=\"sample_prior\")\n",
      "s_post = dl.Function(Vh[PARAMETER], name=\"sample_post\")\n",
      "\n",
      "range_pr = 2*math.sqrt( pr_pw_variance.max() )\n",
      "ps_max   = 2*math.sqrt( post_pw_variance.max() ) + posterior.mean.max()\n",
      "ps_min   = -2*math.sqrt( post_pw_variance.max() ) + posterior.mean.min()\n",
      "\n",
      "for i in range(nsamples):\n",
      "    noise.set_local( np.random.randn( noise_size ) )\n",
      "    posterior.sample(noise, s_prior.vector(), s_post.vector())\n",
      "    plt.figure(figsize=(15,5))\n",
      "    nb.plot(s_prior, subplot_loc=121,mytitle=\"Prior sample\", vmin=-range_pr, vmax=range_pr)\n",
      "    nb.plot(s_post, subplot_loc=122,mytitle=\"Posterior sample\", vmin=ps_min, vmax=ps_max)\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}